#!/usr/bin/env python3
# =================================================================================================
#
# Regenerate ANTLR parser from grammar.
# Also generates the lexical token constants (required by both ANTLR and the scala lexer).
#
# For the purpose of running this script from a Gradle task the following can be assumed:
#
# Input dependencies:
# - regen.py (this file, obviously)
# - SVParser.g4
#
# Outputs (see below for paths):
# - javaDir/*
# - scalaDir/*
#
# Iow the only input is the grammar and the outputs are all written to java/scala source
# directories that only contains code generated by this script.
#
# TODO: do this from build.gradle instead.
#
# =================================================================================================
import os
import subprocess

# Package and dir for generated java/scala code.
package = "com.github.svstuff.systemverilog.generated"
javaDir = os.path.join("src/main/java", package.replace('.', '/'))
scalaDir = os.path.join("src/main/scala", package.replace('.', '/'))

if not os.path.isdir(javaDir):
    raise Exception("Directory for generated java sources does not exist")
if not os.path.isdir(scalaDir):
    raise Exception("Directory for generated scala sources does not exist")

# NOTE: running this script will overwrite the following files:
antlrLexerTokens = os.path.join(javaDir, "SVLexer.tokens")
scalaLexerTokens = os.path.join(scalaDir, "LexerTokens.scala")

# List of lexical token types. ANTLR needs a tokens file with such
# a list (including numberical value), and this must correspond to what
# the actual lexer produces as well.

# Special-cased tokens
tokens_special = [
    # error token, produced when the lexer finds an invalid token
    ("ERROR",               ""),
    ("LIT_STRING",          "string-literal"),
    ("LIT_STRING_DPI",      "DPI"),
    ("LIT_STRING_DPI_C",    "DPI-C"),
    ("LIT_NUM",             "number-literal"),
    ("LIT_UNBASED_UNSIZED", "unbased-unsized-literal"),
    ("LIT_TIME",            "time-literal"),
    ("ID",                  "identifier"),
    ("SYSTEM_ID",           "system-identifier"),
    ("TIMESCALE",           "`timescale"),
    ("DOLLAR_UNIT",         "$unit"),
    ("DOLLAR_ROOT",         "$root"),
    ("DOLLAR_FATAL",        "$fatal"),
    ("DOLLAR_ERROR",        "$error"),
    ("DOLLAR_WARNING",      "$warning"),
    ("DOLLAR_INFO",         "$info"),
    ("DOLLAR_SETUP",        "$setup"),
    ("DOLLAR_HOLD",         "$hold"),
    ("DOLLAR_SETUPHOLD",    "$setuphold"),
    ("DOLLAR_RECOVERY",     "$recovery"),
    ("DOLLAR_REMOVAL",      "$removal"),
    ("DOLLAR_RECREM",       "$recrem"),
    ("DOLLAR_SKEW",         "$skew"),
    ("DOLLAR_TIMESKEW",     "$timeskew"),
    ("DOLLAR_FULLSKEW",     "$fullskew"),
    ("DOLLAR_PERIOD",       "$period"),
    ("DOLLAR_WIDTH",        "$width"),
    ("DOLLAR_NOCHANGE",     "$nochange"),
    ("KW_1STEP",            "1step"),
]

# Operators and other similar terminals, which are lexed greedily.
# Haven't spent much energy in naming these.
tokens_operators = [
    ("DOLLAR",              "$"),
    ("HASH",                "#"),
    ("HASH2",               "##"),
    ("HASH_SUB_HASH",       "#-#"),
    ("HASH_EQ_HASH",        "#=#"),
    ("AT_SIGN",             "@"),
    ("APOSTROPHE",          "'"),
    ("DOT",                 "."),
    ("COLON",               ":"),
    ("COLON_EQ",            ":="),
    ("COLON_DIV",           ":/"),
    ("COLON2",              "::"),
    ("SEMI",                ";"),
    ("COMMA",               ","),
    ("LPAREN",              "("),
    ("RPAREN",              ")"),
    ("LSQUARE",             "["),
    ("RSQUARE",             "]"),
    ("LCURLY",              "{"),
    ("RCURLY",              "}"),
    ("QUE",                 "?"),
    ("NOT",                 "!"),
    ("NOT_EQ",              "!="),
    ("NOT_EQ2",             "!=="),
    ("NOT_EQ_Q",            "!=?"),
    ("MOD",                 "%"),
    ("MOD_EQ",              "%="),
    ("AND",                 "&"),
    ("AND2",                "&&"),
    ("AND3",                "&&&"),
    ("AND_EQ",              "&="),
    ("MUL",                 "*"),
    ("MUL2",                "**"),
    ("MUL_EQ",              "*="),
    ("MUL_GT",              "*>"),
    ("ADD",                 "+"),
    ("ADD2",                "++"),
    ("ADD_EQ",              "+="),
    ("ADD_COLON",           "+:"),
    ("SUB",                 "-"),
    ("SUB2",                "--"),
    ("SUB_EQ",              "-="),
    ("SUB_GT",              "->"),
    ("SUB_GT2",             "->>"),
    ("SUB_COLON",           "-:"),
    ("DIV",                 "/"),
    ("DIV_EQ",              "/="),
    ("LT",                  "<"),
    ("LT_SUB_GT",           "<->"),
    ("LT2",                 "<<"),
    ("LT3",                 "<<<"),
    ("LT3_EQ",              "<<<="),
    ("LT2_EQ",              "<<="),
    ("LT_EQ",               "<="),
    ("EQ",                  "="),
    ("EQ2",                 "=="),
    ("EQ_GT",               "=>"),
    ("EQ3",                 "==="),
    ("EQ2_Q",               "==?"),
    ("GT",                  ">"),
    ("GT_EQ",               ">="),
    ("GT2",                 ">>"),
    ("GT2_EQ",              ">>="),
    ("GT3",                 ">>>"),
    ("GT3_EQ",              ">>>="),
    ("XOR",                 "^"),
    ("XOR_EQ",              "^="),
    ("XOR_INV",             "^~"),
    ("OR",                  "|"),
    ("OR_EQ",               "|="),
    ("OR2",                 "||"),
    ("OR_SUB_GT",           "|->"),
    ("OR_EQ_GT",            "|=>"),
    ("INV",                 "~"),
    ("INV_AND",             "~&"),
    ("INV_XOR",             "~^"),
    ("INV_OR",              "~|"),
]


def get_keyword_tokens():
    import sv_keywords
    tokens = []
    for kw in sv_keywords.keywords:
        tokname = "KW_{}".format(kw.upper())
        tokens.append((tokname, kw))
    return tokens


def regen():
    print("Running ANTLR and generating parser java source...")

    tokens_keywords = get_keyword_tokens()

    tokens = tokens_special + tokens_operators + tokens_keywords

    # start from 1, since this appears to be what ANTLR generates from its own
    # lexers.
    startIndex = 1

    # generate ANTLR tokens file
    try:
        os.makedirs(os.path.dirname(antlrLexerTokens))
    except:
        pass

    with open(antlrLexerTokens, "w") as f:
        i = startIndex
        for ttype, _ in tokens:
            f.write("{} = {}\n".format(ttype, i))
            i += 1

    with open(scalaLexerTokens, "w") as f:
        # generate scala lexer constants
        f.write("// NOTE: this file is automatically generated.\n")
        f.write("package {}\n".format(package))
        f.write("\n")
        # generate scala singleton object with mapping from token type to
        # string
        f.write("object LexerTokens {\n")
        f.write("\n")
        i = startIndex
        for ttype, _ in tokens:
            f.write("  final val {} = {}\n".format(ttype, i))
            i += 1
        f.write("\n")
        f.write("  val tokenNames = Array(\n")
        for i in range(startIndex):
            f.write('    "",\n')
        for toktype, _ in tokens:
            f.write('    "{}",\n'.format(toktype))
        f.write('    "DUMMY")\n')
        f.write("\n")
        f.write("  val tokenConstText = Array(\n")
        for i in range(startIndex):
            f.write('    "",\n')
        for _, text in tokens:
            f.write('    "{}",\n'.format(text))
        f.write('    "DUMMY")\n')
        f.write("\n")
        f.write("  val keywords = Map(\n")
        for kw_index, (toktype, kw) in enumerate(tokens_keywords):
            f.write('    "{}" -> {}'.format(kw, toktype))
            if kw_index == len(tokens_keywords) - 1:
                f.write(')\n')
            else:
                f.write(',\n')
        f.write("\n")
        f.write("  val operators = Map(\n")
        for index, (toktype, op) in enumerate(tokens_operators):
            f.write('    "{}" -> {}'.format(op, toktype))
            if index == len(tokens_operators) - 1:
                f.write(')\n')
            else:
                f.write(',\n')
        f.write("\n")
        f.write('  val operatorPattern = """(?s)(')
        ops_by_length = sorted(
            [op for (_, op) in tokens_operators], key=lambda x: len(x), reverse=True)
        max_length = 0
        for index, op in enumerate(ops_by_length):
            max_length = max(max_length, len(op))
            f.write('\Q{}\E'.format(op))
            if index == len(tokens_operators) - 1:
                f.write(').*""".r\n')
            else:
                f.write('|')
        f.write('  val operatorMaxLength = {}\n'.format(max_length))
        f.write("\n")
        f.write("}\n")

    # run ANTLR on the grammar
    subprocess.check_call(
        "java -Xmx2g -jar lib/antlr-4.4-complete.jar -atn -visitor -o {} SVParser.g4".format(javaDir), shell=True)

if __name__ == "__main__":
    regen()
